# Data settings
data:
  tickers:
    - 'CDR.WA'    # CD Projekt (Polska)
    - 'PKO.WA'    # PKO BP (Polska)
    - 'AAPL'      # Apple (USA)
    - 'MSFT'      # Microsoft (USA)
    - 'TSLA'      # Tesla (USA)
    - 'GOOGL'     # Alphabet (USA)
    - 'AMZN'      # Amazon (USA)
    - 'BMW.DE'    # BMW (Niemcy)
    - 'SIE.DE'    # Siemens (Niemcy)
    - 'SAN.PA'    # Sanofi (Francja)
    - 'TTE.PA'    # TotalEnergies (Francja)
    - 'BP.L'      # BP (Wielka Brytania)
    - 'HSBA.L'    # HSBC (Wielka Brytania)
    - '7203.T'    # Toyota (Japonia)
    - '9984.T'    # SoftBank (Japonia)
    - '005930.KS' # Samsung (Korea Południowa)
    - '2330.TW'   # TSMC (Tajwan)
    - 'BABA'      # Alibaba (Chiny)
    - 'JD'        # JD.com (Chiny)
    - 'INFY.NS'   # Infosys (Indie)
    - 'RELIANCE.NS' # Reliance Industries (Indie)
  years: 3
  raw_data_path: 'data/stock_data.csv'
  processed_data_path: 'data/processed_dataset'

# Model settings
model:
  max_prediction_length: 60  # 1 day to 2 months
  min_encoder_length: 100
  hidden_size: 128
  attention_head_size: 8
  dropout: 0.2
  learning_rate: 0.01
  lstm_layers: 3
  loss: QuantileLoss  # Używamy QuantileLoss
  use_quantile_loss: true  # Włącz QuantileLoss
  quantiles: [0.1, 0.5, 0.9]  # Kwantyle dla predykcji (dolny, mediana, górny)

  min_hidden_size: 64    # Minimalna wielkość warstwy ukrytej dla Optuna
  max_hidden_size: 128   # Maksymalna wielkość warstwy ukrytej dla Optuna
  min_lstm_layers: 3      # Minimalna liczba warstw LSTM dla Optuna
  max_lstm_layers: 5      # Maksymalna liczba warstw LSTM dla Optuna
  min_attention_head_size: 4  # Minimalna liczba głów uwagi dla Optuna
  max_attention_head_size: 8  # Maksymalna liczba głów uwagi dla Optuna

# Training settings
training:
  max_epochs: 50
  batch_size: 32
  early_stopping_patience: 8
  optuna_trials: 10  # Number of trials for hyperparameter optimization
  reduce_lr_patience: 4  # Cierpliwość dla redukcji learning rate
  reduce_lr_factor: 0.5  # Współczynnik redukcji learning rate

# Paths
paths:
  checkpoint_path: 'models/best_tft_checkpoint.pth'  # Ścieżka do najlepszego checkpointu
  model_save_path: 'models/full_model.pth'  # Ścieżka do zapisu całego modelu