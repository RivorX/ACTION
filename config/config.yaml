# Data settings
data:
  tickers:
    - 'CDR.WA'    # CD Projekt (Polska, gry)
    - 'PKO.WA'    # PKO BP (Polska, bankowość)
    - 'AAPL'      # Apple (USA, technologia)
    - 'MSFT'      # Microsoft (USA, technologia)
    - 'TSLA'      # Tesla (USA, motoryzacja)
    - 'GOOGL'     # Alphabet (USA, technologia)
    - 'AMZN'      # Amazon (USA, e-commerce)
    - 'BMW.DE'    # BMW (Niemcy, motoryzacja)
    - 'SIE.DE'    # Siemens (Niemcy, przemysł)
    - 'SAN.PA'    # Sanofi (Francja, farmacja)
    - 'TTE.PA'    # TotalEnergies (Francja, energia)
    - 'BP.L'      # BP (Wielka Brytania, energia)
    - 'HSBA.L'    # HSBC (Wielka Brytania, bankowość)
    - '7203.T'    # Toyota (Japonia, motoryzacja)
    - '9984.T'    # SoftBank (Japonia, technologia)
    - '005930.KS' # Samsung (Korea Południowa, technologia, zmieniono z SSNLF)
    - '2330.TW'   # TSMC (Tajwan, półprzewodniki)
    - 'BABA'      # Alibaba (Chiny, e-commerce)
    - 'JD'        # JD.com (Chiny, e-commerce)
    - 'INFY.NS'   # Infosys (Indie, IT)
    - 'RELIANCE.NS' # Reliance Industries (Indie, energia/konglomerat)
    - 'NKE'       # Nike (USA, odzież)
    - 'JPM'       # JPMorgan Chase (USA, bankowość)
    - 'XOM'       # Exxon Mobil (USA, energia)
    - 'PFE'       # Pfizer (USA, farmacja)
    - 'VOW3.DE'   # Volkswagen (Niemcy, motoryzacja)
    - 'RNO.PA'    # Renault (Francja, motoryzacja)
    - 'NG.L'      # National Grid (Wielka Brytania, energia)
    - 'TM'        # Toyota (USA, motoryzacja)
    - 'SONY'      # Sony (Japonia, technologia)
    - 'TCTZF'     # Tencent (Chiny, technologia)
    - 'HDB'       # HDFC Bank (Indie, bankowość)
  years: 3
  raw_data_path: 'data/stock_data.csv'
  processed_data_path: 'data/processed_dataset'

# Model settings
model:
  max_prediction_length: 60
  min_encoder_length: 150
  hidden_size: 128
  attention_head_size: 8
  dropout: 0.3
  learning_rate: 0.001
  lstm_layers: 5
  loss: QuantileLoss
  use_quantile_loss: true
  quantiles: [0.1, 0.5, 0.9]
  min_hidden_size: 64
  max_hidden_size: 256
  min_lstm_layers: 3
  max_lstm_layers: 5
  min_attention_head_size: 4
  max_attention_head_size: 12

# Training settings
training:
  max_epochs: 100
  batch_size: 48
  early_stopping_patience: 10
  optuna_trials: 30
  reduce_lr_patience: 5
  reduce_lr_factor: 0.5

# Paths
paths:
  checkpoint_path: 'models/best_tft_checkpoint.pth'
  model_save_path: 'models/full_model.pth'